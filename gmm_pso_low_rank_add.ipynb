{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cloud data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.mixture._gaussian_mixture import _compute_precision_cholesky\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Cloud dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_data_name = 'data/cloud.data'\n",
    "with open(cloud_data_name) as f:\n",
    "    cloud_data = pd.DataFrame([item.split() for item in f.readlines()])\n",
    "cloud_data = cloud_data.astype(float).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(cloud_data)\n",
    "cloud_data = scaler.transform(cloud_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Breast Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_data_name = 'data/wdbc.data'\n",
    "with open(bc_data_name) as f:\n",
    "    data_bc = pd.DataFrame([item.split(',')[2:] for item in f.readlines()])\n",
    "data_bc = data_bc.astype(float).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data_bc)\n",
    "data_bc = scaler.transform(data_bc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading LandSat Satelite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_trn_data_name = 'data/sat.trn'\n",
    "\n",
    "with open(sat_trn_data_name) as f:\n",
    "    data_sat_trn = pd.DataFrame([item.split(' ')[:36] for item in f.readlines()])\n",
    "data_sat_trn = data_sat_trn.astype(float).to_numpy()\n",
    "\n",
    "sat_tst_data_name = 'data/sat.tst'\n",
    "with open(sat_tst_data_name) as f:\n",
    "    data_sat_tst = pd.DataFrame([item.split(' ')[:36] for item in f.readlines()])\n",
    "data_sat_tst = data_sat_tst.astype(float).to_numpy()\n",
    "\n",
    "data_sat = np.concatenate([data_sat_trn, data_sat_tst], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data_sat)\n",
    "data_sat = scaler.transform(data_sat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(20, 20), dpi=80)\n",
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "\n",
    "def plot_particle_trajectories(trajectories, best_em):\n",
    "    for i in range(len(trajectories)):\n",
    "        plt.plot(list(range(len(trajectories[i]))), trajectories[i])\n",
    "    plt.plot(list(range(len(trajectories[i]))), [best_em for i in range(len(trajectories[i]))], linewidth=3, c='black')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSO with low rank PD addition method for GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_delta_parametrize(data, log_file_name, n_components=10, amplitude=0.05, rank=10, init_scale=1):\n",
    "    # write PSO and k means algo\n",
    "    n_particles = 25\n",
    "    max_i = 200\n",
    "    w = 0.5\n",
    "    r_1 = 0.6\n",
    "    r_2 = 0.8\n",
    "    r_1_chol = 0.42\n",
    "    r_2_chol = 0.57\n",
    "    r_1_w = 0.42\n",
    "    r_2_w = 0.57\n",
    "    data_dim = data.shape[1]\n",
    "    max_overall_iter = 100\n",
    "\n",
    "    if rank > data_dim:\n",
    "        rank = data_dim\n",
    "\n",
    "    particle_trajectories = [[] for i in range(n_particles)]\n",
    "    \n",
    "    criterions = [0 for i in range(n_particles)]\n",
    "    from functools import partial\n",
    "\n",
    "    with open(log_file_name, 'w+') as f:\n",
    "        \n",
    "            \n",
    "        v_weights = np.random.uniform(-1, 1, size=(n_particles, n_components))\n",
    "        v_delta_means = np.random.uniform(-1, 1, size=(n_particles, n_components, data_dim))\n",
    "        v_delta_diag_prec = np.random.uniform(-1, 1, size=(n_particles, n_components, data_dim))\n",
    "        v_delta_param_prec = np.random.uniform(-1, 1, size=(n_particles, n_components, rank, data_dim))\n",
    "\n",
    "        delta_means = np.zeros((n_particles, n_components, data_dim))\n",
    "        weights = np.zeros((n_particles, n_components))\n",
    "        delta_diag_prec = np.zeros((n_particles, n_components, data_dim))\n",
    "        delta_param_prec = np.zeros((n_particles, n_components, rank, data_dim))\n",
    "\n",
    "        # one BIG GMM init\n",
    "\n",
    "        gmm = GaussianMixture(n_components=n_components, covariance_type='full', init_params='random', n_init=200)\n",
    "        gmm.fit(data)\n",
    "\n",
    "        base_chol = gmm.precisions_cholesky_\n",
    "\n",
    "        basic_prec = np.zeros_like(gmm.precisions_cholesky_)\n",
    "        for i in range(gmm.precisions_cholesky_.shape[0]):\n",
    "            basic_prec[i] = gmm.precisions_cholesky_[i] @ gmm.precisions_cholesky_[i].T\n",
    "\n",
    "        FB_base_chol = np.zeros_like(gmm.precisions_cholesky_)\n",
    "        for i in range(gmm.precisions_cholesky_.shape[0]):\n",
    "            FB_base_chol[i] = np.linalg.cholesky(basic_prec[i])\n",
    "\n",
    "\n",
    "        basic_means = gmm.means_\n",
    "        \n",
    "\n",
    "        def is_pos_def(x):\n",
    "            return np.all(np.linalg.eigvals(x) > 0)\n",
    "\n",
    "        def log_likelihood_gmm_parametrized(weights, delta_means, delta_diag, delta_rank_params):\n",
    "            means = basic_means + delta_means\n",
    "\n",
    "            prec_matr = basic_prec\n",
    "            addition = [0 for i in range(n_components)]\n",
    "\n",
    "            cholesky = np.zeros_like(prec_matr)\n",
    "            \n",
    "            for i in range(n_components):\n",
    "                addition[i]  = np.diag(delta_diag[i] ** 2)\n",
    "\n",
    "                prec_matr[i] += np.diag(delta_diag[i] ** 2)\n",
    "\n",
    "                rank = delta_param_prec.shape[-2]\n",
    "                for k in range(rank):\n",
    "                    prec_matr[i] += delta_rank_params[i][k] @ delta_rank_params[i][k].T\n",
    "                    addition[i] += delta_rank_params[i][k] @ delta_rank_params[i][k].T\n",
    "\n",
    "                cholesky[i] = np.linalg.cholesky(prec_matr[i])\n",
    "\n",
    "            # for i in range(n_components):\n",
    "            #     print('Norm of addiiton to precision matix', np.linalg.norm(addition[i]))\n",
    "            # print('Delta of norms of cholesky before adidtion and after', np.linalg.norm(prec_matr - cholesky @ ch))\n",
    "            # print('Data type form precision matrix', prec_matr.dtype)\n",
    "\n",
    "\n",
    "            gmm_init = GaussianMixture(n_components=weights.shape[0], covariance_type='full', weights_init=weights, means_init=means)\n",
    "            gmm_init.weights_ = weights\n",
    "            gmm_init.means_ = means\n",
    "            gmm_init.precisions_cholesky_ = cholesky\n",
    "\n",
    "            base_gmm = GaussianMixture(n_components=weights.shape[0], covariance_type='full', weights_init=weights, means_init=means)\n",
    "            base_gmm.weights_ = weights\n",
    "            base_gmm.means_ = means\n",
    "            base_gmm.precisions_cholesky_ = base_chol\n",
    "            \n",
    "            base_gmm_fb_chol = GaussianMixture(n_components=weights.shape[0], covariance_type='full', weights_init=weights, means_init=means)\n",
    "            base_gmm_fb_chol.weights_ = weights\n",
    "            base_gmm_fb_chol.means_ = means\n",
    "            base_gmm_fb_chol.precisions_ = base_chol ** 2\n",
    "            # base_gmm_fb_chol.precisions_cholesky_ = np.linalg.cholesky(base_chol ** 2)\n",
    "\n",
    "            #print('Score of GMM with very first cholesky ', base_gmm.score(data))\n",
    "            #print('Score of model with very basic cholesky after forward/backward decompose', base_gmm_fb_chol.score(data))\n",
    "            #print('Score of GMM with OK cholesky after all operation inside the fucntion ', gmm_init.score(data))\n",
    "            return gmm_init.score(data)\n",
    "\n",
    "        # randomly init delta\n",
    "        for i in range(n_particles):\n",
    "            delta_means[i] = np.random.normal(0, init_scale, size=gmm.means_.shape)\n",
    "            weights[i] = gmm.weights_\n",
    "            delta_diag_prec[i] = np.random.normal(0, init_scale, size=delta_diag_prec[i].shape)\n",
    "            delta_param_prec[i] = np.random.normal(0, init_scale, size=delta_param_prec[i].shape)\n",
    "\n",
    "        p_weights = np.copy(weights)\n",
    "        p_delta_means = np.copy(delta_means)\n",
    "        p_delta_diag_prec = np.copy(delta_diag_prec)\n",
    "        p_delta_param_prec = np.copy(delta_param_prec)\n",
    "\n",
    "        g_weights = np.copy(weights[0])\n",
    "        g_delta_means = np.copy(delta_means[0])\n",
    "        g_delta_diag_prec = np.copy(delta_diag_prec[0])\n",
    "        g_delta_param_prec = np.copy(delta_param_prec[0])\n",
    "\n",
    "        for i in range(n_particles):\n",
    "            if log_likelihood_gmm_parametrized(p_weights[i], p_delta_means[i], p_delta_diag_prec[i], p_delta_param_prec[i]) > log_likelihood_gmm_parametrized(g_weights, g_delta_means, g_delta_diag_prec, g_delta_param_prec):\n",
    "                g_weights = np.copy(p_weights[i])\n",
    "                g_delta_means = np.copy(p_delta_means[i])\n",
    "                g_delta_diag_prec = np.copy(p_delta_diag_prec[i])\n",
    "                g_delta_param_prec = np.copy(p_delta_param_prec[i])\n",
    "        \n",
    "        f.write(f'Best initial log likelihood {log_likelihood_gmm_parametrized(g_weights, g_delta_means, g_delta_diag_prec, g_delta_param_prec)}\\n')\n",
    "        # Data for the plotting\n",
    "        best_loglikelihood_init = log_likelihood_gmm_parametrized(g_weights, g_delta_means, g_delta_diag_prec, g_delta_param_prec)\n",
    "        pso_best_loglikelihood = []\n",
    "\n",
    "        iter_n = -1\n",
    "        while (iter_n < max_i):\n",
    "\n",
    "            f.write(f'Iter {iter_n}\\n')\n",
    "\n",
    "            # reinit GMM here\n",
    "            # in case it is not the first iteration\n",
    "\n",
    "            gmm_reinit_list = []\n",
    "\n",
    "            # if (iter_n >= 0):\n",
    "            #     for i in range(n_particles):\n",
    "            #        gmm_reinit_list.append(reinit_gmm(weights[i], delta_means[i], delta_diag_prec[i], delta_param_prec[i]))\n",
    "\n",
    "            # update personal best\n",
    "            for i in range(n_particles):\n",
    "                # f.write(f'Particle {i} loglikelihood after reinit: {gmm_reinit_list[i].score}\\n')\n",
    "                criterions[i] = log_likelihood_gmm_parametrized(weights[i], delta_means[i], delta_diag_prec[i], delta_param_prec[i])\n",
    "                particle_trajectories[i].append(criterions[i])\n",
    "                if log_likelihood_gmm_parametrized(p_weights[i], p_delta_means[i], p_delta_diag_prec[i], p_delta_param_prec[i]) < criterions[i]:\n",
    "                    p_weights[i] = np.copy(weights[i])\n",
    "                    p_delta_means[i] = np.copy(delta_means[i])\n",
    "                    p_delta_diag_prec[i] = np.copy(delta_diag_prec[i])\n",
    "                    p_delta_param_prec[i] = np.copy(p_delta_param_prec[i])\n",
    "        \n",
    "            g_new_weights = np.copy(g_weights)\n",
    "            g_new_delta_means = np.copy(g_delta_means)\n",
    "            g_new_delta_diag_prec = np.copy(g_delta_diag_prec)\n",
    "            g_new_delta_param_prec = np.copy(g_delta_param_prec)\n",
    "\n",
    "            # updating global best\n",
    "            for i in range(n_particles):\n",
    "                if log_likelihood_gmm_parametrized(p_weights[i], p_delta_means[i], p_delta_diag_prec[i], p_delta_param_prec[i]) > log_likelihood_gmm_parametrized(g_new_weights, g_new_delta_means, g_new_delta_diag_prec, g_new_delta_param_prec):\n",
    "                    g_new_weights = np.copy(p_weights[i])\n",
    "                    g_new_delta_means = np.copy(p_delta_means[i])\n",
    "                    g_new_delta_diag_prec = np.copy(p_delta_diag_prec[i])\n",
    "                    g_new_delta_param_prec = np.copy(p_delta_param_prec[i])\n",
    "                \n",
    "\n",
    "            # if there are no imprevements\n",
    "            delta_global = log_likelihood_gmm_parametrized(g_new_weights, g_new_delta_means, g_new_delta_diag_prec, g_new_delta_param_prec) - log_likelihood_gmm_parametrized(g_weights, g_delta_means, g_delta_diag_prec, g_delta_param_prec)\n",
    "            \n",
    "            if delta_global > 0:\n",
    "                g_weights = np.copy(g_new_weights)\n",
    "                g_delta_means = np.copy(g_new_delta_means)\n",
    "                g_delta_diag_prec = np.copy(g_new_delta_diag_prec)\n",
    "                g_delta_param_prec = np.copy(g_new_delta_param_prec)\n",
    "\n",
    "            # if delta_global > 1e-5:\n",
    "            #     iter_n = 0\n",
    "            # else:\n",
    "            #     iter_n += 1\n",
    "            iter_n += 1\n",
    "            \n",
    "            # PSO update\n",
    "            for j in range(n_particles):\n",
    "                c_1 = np.random.uniform(0, 1)\n",
    "                c_2 = np.random.uniform(0, 1)\n",
    "\n",
    "                # weights\n",
    "                # deleted v_weights * w to exclude probability that weights will go below zero\n",
    "                v_weights[j] = c_1 * r_1_w * (p_weights[j] - weights[j]) + c_2 * r_2_w * (g_weights - weights[j])\n",
    "                weights[j] += amplitude * v_weights[j]\n",
    "\n",
    "                v_delta_means[j] = c_1 * r_1 * (p_delta_means[j] - delta_means[j]) + c_2 * r_2 * (g_delta_means - delta_means[j])\n",
    "                delta_means[j] += amplitude * v_delta_means[j]\n",
    "\n",
    "                v_delta_diag_prec[j] = c_1 * r_1 * (p_delta_diag_prec[j] - delta_diag_prec[j]) + c_2 * r_2 * (g_delta_diag_prec - delta_diag_prec[j])\n",
    "                delta_diag_prec[j] += amplitude * v_delta_diag_prec[j]\n",
    "\n",
    "                v_delta_param_prec[j] = c_1 * r_1 * (p_delta_param_prec[j] - delta_param_prec[j]) + c_2 * r_2 * (g_delta_param_prec - delta_param_prec[j])\n",
    "                delta_param_prec[j] +=  amplitude * v_delta_param_prec[j]\n",
    "                \n",
    "            f.write(f'Best log likelihood {log_likelihood_gmm_parametrized(g_weights, g_delta_means, g_delta_diag_prec, g_delta_param_prec)}\\n')\n",
    "            pso_best_loglikelihood.append(log_likelihood_gmm_parametrized(g_weights, g_delta_means, g_delta_diag_prec, g_delta_param_prec))\n",
    "\n",
    "            f.flush()\n",
    "        \n",
    "        n_inits_for_reference = max_i * n_particles\n",
    "\n",
    "        # gmm_random_init = GaussianMixture(n_components=n_components, covariance_type='full', init_params='random', n_init=n_inits_for_reference)\n",
    "        # gmm_random_init.fit(data)\n",
    "        # f.write(f'Best log likelihood for random init GMM with {max_i * n_particles} initalizations: {gmm_random_init.score(data)}')\n",
    "        # large_gmm_init_score = gmm_random_init.score(data)\n",
    "\n",
    "        large_gmm_init_score = 0\n",
    "    return pso_best_loglikelihood, best_loglikelihood_init, large_gmm_init_score, particle_trajectories\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_scale_range = [0.001, 0.0005]\n",
    "for init_scale  in init_scale_range:\n",
    "\n",
    "    log_file_name = f'pso_low_rank_parametrize_init_scale_{init_scale}.log'\n",
    "\n",
    "    run_experiment_delta_parametrize(cloud_data, log_file_name, n_components=10, amplitude=0.05, rank=5, init_scale=init_scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.035450343122715\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gmm = GaussianMixture(n_components=10, covariance_type='full', init_params='random', n_init=500)\n",
    "gmm.fit(cloud_data)\n",
    "\n",
    "base_chol = gmm.precisions_cholesky_\n",
    "\n",
    "basic_prec = gmm.precisions_cholesky_ ** 2\n",
    "basic_means = gmm.means_\n",
    "basic_cov = gmm.covariances_\n",
    "print(gmm.score(cloud_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('ml_base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9bfb2d55de37064be05f21953ecbbe4c1eff451757a1067937cd706768d7c377"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
